{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Age', 'Cabin', 'Embarked', 'Fare', 'Name', 'Parch', 'PassengerId', 'Pclass', 'Sex', 'SibSp', 'Survived', 'Ticket']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Name</th>\n",
       "      <th>Parch</th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Sex</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Ticket</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>22.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>Braund, Mr. Owen Harris</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>male</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>A/5 21171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>38.0</td>\n",
       "      <td>C85</td>\n",
       "      <td>C</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>female</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>PC 17599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>26.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>Heikkinen, Miss. Laina</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>female</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>STON/O2. 3101282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>35.0</td>\n",
       "      <td>C123</td>\n",
       "      <td>S</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>female</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>113803</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>35.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>Allen, Mr. William Henry</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>male</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>373450</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Age Cabin Embarked     Fare  \\\n",
       "0  22.0   NaN        S   7.2500   \n",
       "1  38.0   C85        C  71.2833   \n",
       "2  26.0   NaN        S   7.9250   \n",
       "3  35.0  C123        S  53.1000   \n",
       "4  35.0   NaN        S   8.0500   \n",
       "\n",
       "                                                Name  Parch  PassengerId  \\\n",
       "0                            Braund, Mr. Owen Harris      0            1   \n",
       "1  Cumings, Mrs. John Bradley (Florence Briggs Th...      0            2   \n",
       "2                             Heikkinen, Miss. Laina      0            3   \n",
       "3       Futrelle, Mrs. Jacques Heath (Lily May Peel)      0            4   \n",
       "4                           Allen, Mr. William Henry      0            5   \n",
       "\n",
       "   Pclass     Sex  SibSp  Survived            Ticket  \n",
       "0       3    male      1       0.0         A/5 21171  \n",
       "1       1  female      1       1.0          PC 17599  \n",
       "2       3  female      0       1.0  STON/O2. 3101282  \n",
       "3       1  female      1       1.0            113803  \n",
       "4       3    male      0       0.0            373450  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This is a machine learning project for the titanic Kaggle competition. \n",
    "# https://www.kaggle.com/c/titanic \n",
    "# The goal is to predict whether a passenger survived using the competition data which are mostly socioeconomical. \n",
    "# The training set must be used to train the machine learning instance while the test set is used to obtain the final score.\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from IPython.display import display  # This allow to display dataframe even if they are not the last thing of their cell.\n",
    "\n",
    "# Read data\n",
    "train_df = pd.read_csv('train.csv')\n",
    "test_df = pd.read_csv('test.csv')\n",
    "combine_df = train_df.append(test_df)\n",
    "\n",
    "# Print features\n",
    "print(list(combine_df))\n",
    "# Show small sample of data\n",
    "combine_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 891 entries, 0 to 890\n",
      "Data columns (total 12 columns):\n",
      "PassengerId    891 non-null int64\n",
      "Survived       891 non-null int64\n",
      "Pclass         891 non-null int64\n",
      "Name           891 non-null object\n",
      "Sex            891 non-null object\n",
      "Age            714 non-null float64\n",
      "SibSp          891 non-null int64\n",
      "Parch          891 non-null int64\n",
      "Ticket         891 non-null object\n",
      "Fare           891 non-null float64\n",
      "Cabin          204 non-null object\n",
      "Embarked       889 non-null object\n",
      "dtypes: float64(2), int64(5), object(5)\n",
      "memory usage: 83.6+ KB\n",
      "________________________________________\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 418 entries, 0 to 417\n",
      "Data columns (total 11 columns):\n",
      "PassengerId    418 non-null int64\n",
      "Pclass         418 non-null int64\n",
      "Name           418 non-null object\n",
      "Sex            418 non-null object\n",
      "Age            332 non-null float64\n",
      "SibSp          418 non-null int64\n",
      "Parch          418 non-null int64\n",
      "Ticket         418 non-null object\n",
      "Fare           417 non-null float64\n",
      "Cabin          91 non-null object\n",
      "Embarked       418 non-null object\n",
      "dtypes: float64(2), int64(4), object(5)\n",
      "memory usage: 36.0+ KB\n"
     ]
    }
   ],
   "source": [
    "# See missing data and data type\n",
    "train_df.info()\n",
    "print('_'*40)\n",
    "test_df.info()\n",
    "\n",
    "# There are 891 training cases and 418 test cases. Age and cabin have a large number of NaNs. \n",
    "# Embarked has 2 NaN in the training data and fare has 1 in the testing data.\n",
    "# All those NaN represents unknown values, XGBoost (the most used machine learning tools in Kaggle competition) can handdle them\n",
    "# by itself, but other machine learning classifier needs that those values be replaced or removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Fare</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>891.000000</td>\n",
       "      <td>891.000000</td>\n",
       "      <td>891.000000</td>\n",
       "      <td>714.000000</td>\n",
       "      <td>891.000000</td>\n",
       "      <td>891.000000</td>\n",
       "      <td>891.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>446.000000</td>\n",
       "      <td>0.383838</td>\n",
       "      <td>2.308642</td>\n",
       "      <td>29.699118</td>\n",
       "      <td>0.523008</td>\n",
       "      <td>0.381594</td>\n",
       "      <td>32.204208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>257.353842</td>\n",
       "      <td>0.486592</td>\n",
       "      <td>0.836071</td>\n",
       "      <td>14.526497</td>\n",
       "      <td>1.102743</td>\n",
       "      <td>0.806057</td>\n",
       "      <td>49.693429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.420000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>223.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>20.125000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7.910400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>446.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>28.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>14.454200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>668.500000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>38.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>31.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>891.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>80.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>512.329200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       PassengerId    Survived      Pclass         Age       SibSp  \\\n",
       "count   891.000000  891.000000  891.000000  714.000000  891.000000   \n",
       "mean    446.000000    0.383838    2.308642   29.699118    0.523008   \n",
       "std     257.353842    0.486592    0.836071   14.526497    1.102743   \n",
       "min       1.000000    0.000000    1.000000    0.420000    0.000000   \n",
       "25%     223.500000    0.000000    2.000000   20.125000    0.000000   \n",
       "50%     446.000000    0.000000    3.000000   28.000000    0.000000   \n",
       "75%     668.500000    1.000000    3.000000   38.000000    1.000000   \n",
       "max     891.000000    1.000000    3.000000   80.000000    8.000000   \n",
       "\n",
       "            Parch        Fare  \n",
       "count  891.000000  891.000000  \n",
       "mean     0.381594   32.204208  \n",
       "std      0.806057   49.693429  \n",
       "min      0.000000    0.000000  \n",
       "25%      0.000000    7.910400  \n",
       "50%      0.000000   14.454200  \n",
       "75%      0.000000   31.000000  \n",
       "max      6.000000  512.329200  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# information about the distribution of rhw numerical data\n",
    "train_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>891</td>\n",
       "      <td>891</td>\n",
       "      <td>891</td>\n",
       "      <td>204</td>\n",
       "      <td>889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>891</td>\n",
       "      <td>2</td>\n",
       "      <td>681</td>\n",
       "      <td>147</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>Mangan, Miss. Mary</td>\n",
       "      <td>male</td>\n",
       "      <td>CA. 2343</td>\n",
       "      <td>C23 C25 C27</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>1</td>\n",
       "      <td>577</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>644</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      Name   Sex    Ticket        Cabin Embarked\n",
       "count                  891   891       891          204      889\n",
       "unique                 891     2       681          147        3\n",
       "top     Mangan, Miss. Mary  male  CA. 2343  C23 C25 C27        S\n",
       "freq                     1   577         7            4      644"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# information about categorical data\n",
    "train_df.describe(include=['O'])\n",
    "# Ticket and cabin have a very large number of unique cases. They can't be used at is. Embarked has 3 cases, thus can be used\n",
    "# as dummy variables, while Sex can be one dummy variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ticket\n",
      "110152               86.5000\n",
      "110152               86.5000\n",
      "110152               86.5000\n",
      "110413               79.6500\n",
      "110413               79.6500\n",
      "110413               79.6500\n",
      "110465               52.0000\n",
      "110465               52.0000\n",
      "110564               26.5500\n",
      "110813               75.2500\n",
      "111240               33.5000\n",
      "111320               38.5000\n",
      "111361               57.9792\n",
      "111361               57.9792\n",
      "111369               30.0000\n",
      "111426               26.5500\n",
      "111427               26.5500\n",
      "111428               26.5500\n",
      "112050                0.0000\n",
      "112052                0.0000\n",
      "112053               30.0000\n",
      "112058                0.0000\n",
      "112059                0.0000\n",
      "112277               31.0000\n",
      "112379               39.6000\n",
      "113028               26.5500\n",
      "113043               28.5000\n",
      "113050               26.5500\n",
      "113051               27.7500\n",
      "113055               26.5500\n",
      "                      ...   \n",
      "STON/O 2. 3101274     7.1250\n",
      "STON/O 2. 3101275     7.1250\n",
      "STON/O 2. 3101280     7.9250\n",
      "STON/O 2. 3101285     7.9250\n",
      "STON/O 2. 3101286     7.9250\n",
      "STON/O 2. 3101288     7.9250\n",
      "STON/O 2. 3101289     7.9250\n",
      "STON/O 2. 3101292     7.9250\n",
      "STON/O 2. 3101293     7.9250\n",
      "STON/O 2. 3101294     7.9250\n",
      "STON/O2. 3101271      7.9250\n",
      "STON/O2. 3101279     15.8500\n",
      "STON/O2. 3101279     15.8500\n",
      "STON/O2. 3101282      7.9250\n",
      "STON/O2. 3101283      7.9250\n",
      "STON/O2. 3101290      7.9250\n",
      "SW/PP 751            10.5000\n",
      "W./C. 14258          10.5000\n",
      "W./C. 14263          10.5000\n",
      "W./C. 6607           23.4500\n",
      "W./C. 6607           23.4500\n",
      "W./C. 6608           34.3750\n",
      "W./C. 6608           34.3750\n",
      "W./C. 6608           34.3750\n",
      "W./C. 6608           34.3750\n",
      "W./C. 6609            7.5500\n",
      "W.E.P. 5734          61.1750\n",
      "W/C 14208            10.5000\n",
      "WE/P 5735            71.0000\n",
      "WE/P 5735            71.0000\n",
      "Name: Fare, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Study ticket feature\n",
    "tmp = train_df.set_index('Ticket').sort_index()\n",
    "print(tmp['Fare'])\n",
    "\n",
    "# Ticket numbers and their fare are repeated. The best explanation is that when someone buy places for a group of people, there\n",
    "# is only one ticket, which has only one fare which is for the group. Thus, a more representative informartion might be the fare\n",
    "# per person. It is also possible that giving different ponderation for men, women and children would give a better predictor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Count number of people with same tickets.\n",
    "combine_df = combine_df.join(combine_df.groupby(by='Ticket').size().rename('Ticket_count'),on='Ticket')\n",
    "# Fare most likely means fare of the ticket rather than fare per person. \n",
    "# So I need to create this feature as it might be more useful than fare\n",
    "combine_df['FarePerPerson']=combine_df['Fare']/combine_df['Ticket_count']\n",
    "# The sum of Sibling, spouse, parent and children might be more useful than having Sibling and spouse, then parent and children\n",
    "combine_df['Family'] = combine_df['SibSp'] + combine_df['Parch']\n",
    "# One feature is enough for sex, no need to break into two when doing dummies.\n",
    "combine_df['Sex'] = combine_df['Sex'].map( {'female': 1, 'male': 0} )\n",
    "# PClass is not a real number, let's transform it into dummies.\n",
    "combine_df['Pclass'] = combine_df['Pclass'].replace({1 : '1st',2 : '2nd',3 : '3rd'})\n",
    "\n",
    "# Create title row. I will transform it with get dummies after that\n",
    "# Titles in names are certainly useful, but there are too many instances. So it is useful to combine the rare titles together.\n",
    "# Also titles related to unmarried women (Miss., Mlle.) should be together, idem for those of married women (Mrs. Mme.). \n",
    "# Ms. which is for unknown marital status may go either way.\n",
    "def fun0(x):\n",
    "    tmp = (x.split(',')[1]).split()[0]\n",
    "    if tmp in ['Mr.','Master.','Dr.','Rev.']:\n",
    "        return tmp\n",
    "    elif tmp in ['Miss.', 'Mlle.', 'Ms.']:\n",
    "        return 'Miss.'\n",
    "    elif tmp in ['Mrs.', 'Mme.']:\n",
    "        return 'Mrs.'\n",
    "    else:\n",
    "        return 'Other'\n",
    "combine_df['title'] = combine_df['Name'].apply(fun0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Age Cabin Embarked     Fare  \\\n",
      "0  22.0   NaN        S   7.2500   \n",
      "1  38.0   C85        C  71.2833   \n",
      "2  26.0   NaN        S   7.9250   \n",
      "3  35.0  C123        S  53.1000   \n",
      "4  35.0   NaN        S   8.0500   \n",
      "\n",
      "                                                Name  Parch  PassengerId  \\\n",
      "0                            Braund, Mr. Owen Harris      0            1   \n",
      "1  Cumings, Mrs. John Bradley (Florence Briggs Th...      0            2   \n",
      "2                             Heikkinen, Miss. Laina      0            3   \n",
      "3       Futrelle, Mrs. Jacques Heath (Lily May Peel)      0            4   \n",
      "4                           Allen, Mr. William Henry      0            5   \n",
      "\n",
      "  Pclass  Sex  SibSp        ...                   Ticket Ticket_count  \\\n",
      "0    3rd    0      1        ...                A/5 21171            1   \n",
      "1    1st    1      1        ...                 PC 17599            2   \n",
      "2    3rd    1      0        ...         STON/O2. 3101282            1   \n",
      "3    1st    1      1        ...                   113803            2   \n",
      "4    3rd    0      0        ...                   373450            1   \n",
      "\n",
      "   FarePerPerson  Family  title SameTicketSurvival   LastName tmp1  tmp2  \\\n",
      "0        7.25000       1    Mr.                NaN     Braund    2   0.0   \n",
      "1       35.64165       1   Mrs.                NaN    Cumings    1   1.0   \n",
      "2        7.92500       0  Miss.                NaN  Heikkinen    1   1.0   \n",
      "3       26.55000       1   Mrs.                0.0   Futrelle    2   1.0   \n",
      "4        8.05000       0    Mr.                NaN      Allen    2   1.0   \n",
      "\n",
      "   LastNameSurvival  \n",
      "0               0.0  \n",
      "1               NaN  \n",
      "2               NaN  \n",
      "3               0.0  \n",
      "4               1.0  \n",
      "\n",
      "[5 rows x 21 columns]\n",
      "['Age', 'Embarked', 'Fare', 'Parch', 'Pclass', 'Sex', 'SibSp', 'Survived', 'Ticket_count', 'FarePerPerson', 'Family', 'title', 'SameTicketSurvival', 'LastNameSurvival']\n"
     ]
    }
   ],
   "source": [
    "# Create same ticket survival\n",
    "# People are more likely to die if the people who shared their ticket died.\n",
    "# Especially if there are men and some women or children who shared their ticket died. It's not so true in the opposite direction\n",
    "# since so many men died.\n",
    "tmp1 = (combine_df.groupby(by='Ticket').count())['Survived']\n",
    "tmp2 = (combine_df.groupby(by='Ticket').sum())['Survived']\n",
    "combine_df['tmp1'] = combine_df['Ticket'].replace(to_replace=tmp1)   # Number of non_nan survived with same ticket\n",
    "combine_df['tmp2'] = combine_df['Ticket'].replace(to_replace=tmp2)   # Sum of non_nan survived with same ticket\n",
    "\n",
    "def f(row):\n",
    "    if np.isnan(row['Survived']): # test\n",
    "        if row['tmp1'] == 0 :\n",
    "            val = np.nan\n",
    "        else:\n",
    "            val = row['tmp2'] /row['tmp1']\n",
    "    else: # train\n",
    "        if row['tmp1'] == 1 :\n",
    "            val = np.nan\n",
    "        else:\n",
    "            val = (row['tmp2']-row['Survived']) / (row['tmp1']-1)\n",
    "    return val\n",
    "combine_df['SameTicketSurvival'] = combine_df.apply(f, axis=1)\n",
    "combine_df = combine_df.drop('tmp1',axis=1).drop('tmp2',axis=1)\n",
    "\n",
    "# Create same last name survival\n",
    "# The same is true with last name, as they are likely to be in the same family.\n",
    "combine_df['LastName'] = combine_df['Name'].apply(lambda x : (x.split(',')[0]))\n",
    "tmp1 = (combine_df.groupby(by='LastName').count())['Survived']\n",
    "tmp2 = (combine_df.groupby(by='LastName').sum())['Survived']\n",
    "combine_df['tmp1'] = combine_df['LastName'].replace(to_replace=tmp1)   # Number of non_nan survived with same ticket\n",
    "combine_df['tmp2'] = combine_df['LastName'].replace(to_replace=tmp2)   # Sum of non_nan survived with same ticket\n",
    "combine_df['LastNameSurvival'] = combine_df.apply(f, axis=1)\n",
    "\n",
    "display((combine_df.head())\n",
    "\n",
    "# Remove LastName because there are two many different LastName, so it wouldn't work with get_dummies which is needed for xgboost.\n",
    "# Same for ticket\n",
    "# Same for Cabin, which has also too many missing values.\n",
    "# PassengerID should not add any information as it is arbitrary\n",
    "# Name is useless as everybody as a unique name.\n",
    "combine_df = (combine_df.drop('tmp1',axis=1).drop('tmp2',axis=1).drop('LastName',axis=1).drop('Ticket',axis=1).\n",
    "              drop('PassengerId',axis=1).drop('Cabin',axis=1).drop('Name',axis=1))\n",
    "        \n",
    "display((combine_df.head())\n",
    "\n",
    "print(list(combine_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Age', 'Fare', 'Parch', 'Sex', 'SibSp', 'Survived', 'Ticket_count', 'FarePerPerson', 'Family', 'SameTicketSurvival', 'LastNameSurvival', 'Embarked_C', 'Embarked_Q', 'Embarked_S', 'Pclass_1st', 'Pclass_2nd', 'Pclass_3rd', 'title_Dr.', 'title_Master.', 'title_Miss.', 'title_Mr.', 'title_Mrs.', 'title_Other', 'title_Rev.']\n"
     ]
    }
   ],
   "source": [
    "# Create dummies for every string variable\n",
    "combine_df = pd.get_dummies(combine_df)\n",
    "\n",
    "# Split Other between gender\n",
    "# That might be useful and the other title are mostly separated by gender.\n",
    "print(list(combine_df))\n",
    "combine_df['OtherFemale'] = combine_df['title_Other'] * combine_df['Sex'] + combine_df['title_Dr.'] * combine_df['Sex']\n",
    "combine_df['OtherMale'] = combine_df['title_Other'] - combine_df['OtherFemale']\n",
    "\n",
    "# Split classes between gender\n",
    "# That might be useful since gender is a very important characteristic which interacts with the class since the boosted tree\n",
    "# technic won't allow to always use the Sex characteristic to do a branching.\n",
    "combine_df['1F'] = combine_df['Pclass_1st'] * combine_df['Sex']\n",
    "combine_df['2F'] = combine_df['Pclass_2nd'] * combine_df['Sex']\n",
    "combine_df['3F'] = combine_df['Pclass_3rd'] * combine_df['Sex']\n",
    "combine_df['1M'] = combine_df['Pclass_1st'] - combine_df['1F']\n",
    "combine_df['2M'] = combine_df['Pclass_2nd'] - combine_df['2F']\n",
    "combine_df['3M'] = combine_df['Pclass_3rd'] - combine_df['3F']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Separate in train and test\n",
    "train_df = combine_df[pd.notnull(combine_df['Survived'])]\n",
    "test_df = combine_df[False == pd.notnull(combine_df['Survived'])]\n",
    "\n",
    "x = train_df.drop('Survived',axis=1)\n",
    "y = train_df['Survived']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 522.3922550678253 seconds ---\n",
      "{'colsample_bylevel': 0.666283305643945, 'colsample_bytree': 0.96025711198482, 'gamma': 0.4538320367589998, 'learning_rate': 0.1672241327029708, 'max_depth': 10, 'min_child_weight': 1.0, 'reg_alpha': 0.22779378353622617, 'reg_lambda': 0.6926529168023405, 'subsample': 0.9995550469608545}\n",
      "--- 102.16380286216736 seconds ---\n",
      "11.4 12.7121988657 0.148246962 0.00511404120458\n",
      "             Survived\n",
      "PassengerId          \n",
      "892                 0\n",
      "893                 0\n",
      "894                 0\n",
      "895                 0\n",
      "896                 1\n",
      "897                 0\n",
      "898                 1\n",
      "899                 0\n",
      "900                 1\n",
      "901                 0\n",
      "902                 0\n",
      "903                 0\n",
      "904                 1\n",
      "905                 0\n",
      "906                 1\n",
      "907                 1\n",
      "908                 0\n",
      "909                 0\n",
      "910                 0\n",
      "911                 0\n",
      "912                 0\n",
      "913                 0\n",
      "914                 1\n",
      "915                 0\n",
      "916                 1\n",
      "917                 0\n",
      "918                 1\n",
      "919                 0\n",
      "920                 1\n",
      "921                 0\n",
      "...               ...\n",
      "1280                0\n",
      "1281                0\n",
      "1282                0\n",
      "1283                1\n",
      "1284                1\n",
      "1285                0\n",
      "1286                0\n",
      "1287                1\n",
      "1288                0\n",
      "1289                1\n",
      "1290                0\n",
      "1291                0\n",
      "1292                1\n",
      "1293                0\n",
      "1294                1\n",
      "1295                0\n",
      "1296                0\n",
      "1297                0\n",
      "1298                0\n",
      "1299                0\n",
      "1300                1\n",
      "1301                1\n",
      "1302                1\n",
      "1303                1\n",
      "1304                0\n",
      "1305                0\n",
      "1306                1\n",
      "1307                0\n",
      "1308                0\n",
      "1309                1\n",
      "\n",
      "[418 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "# XGBoost is known to be the best machine learning algorithm for this kind of task.\n",
    "# Hyperopt is a good hyperparameter optimizer.\n",
    "\n",
    "# XGBoost.cv selects the best number of estimators and gives a good cross validation scores.\n",
    "# The 1st step is to find the best parameters using hyperopt and XGBoost.cv. Then, one uses those parameters to train a \n",
    "# classifier with all the data. However, an important difference between the use of XGBoost.cv and the final fitting is that\n",
    "# with XGBoost.cv we split the data in 4/5 training set and 1/5 cross-validation set. Thus, the parameters may not be the best\n",
    "# for the final training. It is particularly the case of n_estimators which has a use uncertainty.\n",
    "\n",
    "import xgboost as xgb\n",
    "from hyperopt import hp, fmin, tpe, STATUS_OK, Trials\n",
    "from time import time\n",
    "from random import randint\n",
    "\n",
    "nrepeat = 20\n",
    "repeat = range(nrepeat)\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "dmatrix = xgb.DMatrix(x.values, y.values)\n",
    "\n",
    "def objective(space):\n",
    "    dmatrix = xgb.DMatrix(x.values, y.values)\n",
    "    param={'learning_rate' : space['learning_rate'],\n",
    "     'max_depth' : int(space['max_depth']),\n",
    "     'min_child_weight' : space['min_child_weight'],\n",
    "        'gamma' : space['gamma'],\n",
    "     'subsample' : space['subsample'],\n",
    "     'colsample_bytree' :space['colsample_bytree'],\n",
    "     'nthread' : -1,\n",
    "     'colsample_bylevel' :space['colsample_bylevel'],\n",
    "        'reg_alpha' :space['reg_alpha'],\n",
    "     'reg_lambda' :space['reg_lambda']  \n",
    "          }\n",
    "    error = 0\n",
    "    for i in repeat:\n",
    "        seed = randint(0, 100000)\n",
    "        cvresult = xgb.cv(param, dmatrix, num_boost_round=1000, nfold=5,\n",
    "                metrics='error', early_stopping_rounds=10, shuffle = True, seed=seed)\n",
    "        error += cvresult['test-error-mean'].tail(1).values[0]\n",
    "    error /= nrepeat\n",
    "\n",
    "    return{'loss':error, 'status': STATUS_OK }\n",
    "\n",
    "\n",
    "space ={\n",
    "        'max_depth': hp.quniform ('max_depth', 1, 14, 1),\n",
    "        'min_child_weight': hp.quniform ('min_child_weight', 1, 10, 1),\n",
    "        'subsample': hp.uniform ('subsample', 0.1, 1.),\n",
    "        'learning_rate' : hp.uniform ('learning_rate', 0.01, 0.5),\n",
    "       'colsample_bytree': hp.uniform ('colsample_bytree', 0.5, 1.),\n",
    "        'colsample_bylevel': hp.uniform ('colsample_bylevel', 0.5, 1.),\n",
    "        'gamma': hp.uniform ('gamma', 0., 1.),\n",
    "    'reg_alpha': hp.uniform ('reg_alpha', 0., 1.),\n",
    "    'reg_lambda' : hp.uniform ('reg_lambda', 0., 1.)\n",
    "    }\n",
    "\n",
    "trials = Trials()\n",
    "start_time = time()\n",
    "best = fmin(fn=objective,\n",
    "            space=space,\n",
    "            algo=tpe.suggest,\n",
    "            max_evals=100,\n",
    "            trials=trials)\n",
    "print(\"--- %s seconds ---\" % (time() - start_time))\n",
    "\n",
    "best['max_depth'] = int(best['max_depth'])\n",
    "print (best)\n",
    "\n",
    "import numpy as np\n",
    "start_time = time()\n",
    "n_estimators = []\n",
    "Error = []\n",
    "for i in range(100):\n",
    "    seed = randint(0, 100000)\n",
    "    cvresult = xgb.cv(best, dmatrix, num_boost_round=1000, nfold=5,\n",
    "                metrics='error', early_stopping_rounds=50, shuffle = True, seed=seed)\n",
    "    n_estimators.append(cvresult.shape[0])\n",
    "    Error.append(cvresult['test-error-mean'].tail(1).values[0])\n",
    "print(\"--- %s seconds ---\" % (time() - start_time))\n",
    "n_estimators_mean = np.mean(n_estimators)\n",
    "n_estimators_std = np.std(n_estimators)\n",
    "Error_mean = np.mean(Error)\n",
    "Error_std = np.std(Error)\n",
    "print(n_estimators_mean, n_estimators_std, Error_mean, Error_std)\n",
    "\n",
    "best1 = best\n",
    "best1['n_estimators'] = int(round(n_estimators_mean))\n",
    "clf = xgb.XGBClassifier(**best1)\n",
    "prediction1 = clf.fit(x,y).predict(test_df.drop('Survived',axis=1))\n",
    "\n",
    "result_df = pd.read_csv('test.csv').drop('Pclass',axis=1).drop('Name',axis=1).drop('Sex',axis=1).drop('Age',axis=1).drop('SibSp',axis=1).drop('Parch',axis=1).drop('Ticket',axis=1).drop('Fare',axis=1).drop('Cabin',axis=1).drop('Embarked',axis=1)\n",
    "result_df['Survived'] = pd.Series(data=prediction1)\n",
    "result_df = result_df.set_index('PassengerId').astype(int)\n",
    "print(result_df)\n",
    "\n",
    "result_df.to_csv(path_or_buf='Result_Titanic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 541.779257774353 seconds ---\n",
      "{'colsample_bylevel': 0.8793854140464347, 'colsample_bytree': 0.5284021992184958, 'gamma': 0.4877440792405143, 'learning_rate': 0.44616929141410355, 'max_depth': 4, 'min_child_weight': 5.0, 'reg_alpha': 0.40433603353443726, 'reg_lambda': 0.2570910174307357, 'subsample': 0.10055864508343322}\n",
      "--- 35.055745363235474 seconds ---\n",
      "3.59 0.939095309327 0.368803328667 0.156626686268\n",
      "             Survived\n",
      "PassengerId          \n",
      "892                 0\n",
      "893                 1\n",
      "894                 0\n",
      "895                 0\n",
      "896                 1\n",
      "897                 0\n",
      "898                 1\n",
      "899                 0\n",
      "900                 1\n",
      "901                 0\n",
      "902                 0\n",
      "903                 0\n",
      "904                 1\n",
      "905                 0\n",
      "906                 1\n",
      "907                 1\n",
      "908                 0\n",
      "909                 0\n",
      "910                 1\n",
      "911                 1\n",
      "912                 0\n",
      "913                 0\n",
      "914                 1\n",
      "915                 0\n",
      "916                 1\n",
      "917                 0\n",
      "918                 1\n",
      "919                 0\n",
      "920                 0\n",
      "921                 0\n",
      "...               ...\n",
      "1280                0\n",
      "1281                0\n",
      "1282                0\n",
      "1283                1\n",
      "1284                1\n",
      "1285                0\n",
      "1286                0\n",
      "1287                1\n",
      "1288                0\n",
      "1289                1\n",
      "1290                0\n",
      "1291                0\n",
      "1292                1\n",
      "1293                0\n",
      "1294                1\n",
      "1295                0\n",
      "1296                0\n",
      "1297                0\n",
      "1298                0\n",
      "1299                0\n",
      "1300                1\n",
      "1301                1\n",
      "1302                1\n",
      "1303                1\n",
      "1304                1\n",
      "1305                0\n",
      "1306                1\n",
      "1307                0\n",
      "1308                0\n",
      "1309                0\n",
      "\n",
      "[418 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "# Try again with logloss rather than accuracy\n",
    "evalmetrics = 'logloss'\n",
    "nrepeat = 20\n",
    "repeat = range(nrepeat)\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "dmatrix = xgb.DMatrix(x.values, y.values)\n",
    "\n",
    "def objective(space):\n",
    "    dmatrix = xgb.DMatrix(x.values, y.values)\n",
    "    param={'learning_rate' : space['learning_rate'],\n",
    "     'max_depth' : int(space['max_depth']),\n",
    "     'min_child_weight' : space['min_child_weight'],\n",
    "        'gamma' : space['gamma'],\n",
    "     'subsample' : space['subsample'],\n",
    "     'colsample_bytree' :space['colsample_bytree'],\n",
    "     'nthread' : -1,\n",
    "     'colsample_bylevel' :space['colsample_bylevel'],\n",
    "        'reg_alpha' :space['reg_alpha'],\n",
    "     'reg_lambda' :space['reg_lambda']  \n",
    "          }\n",
    "    error = 0\n",
    "    for i in repeat:\n",
    "        seed = randint(0, 100000)\n",
    "        cvresult = xgb.cv(param, dmatrix, num_boost_round=1000, nfold=5,\n",
    "                metrics=evalmetrics, early_stopping_rounds=10, shuffle = True, seed=seed)\n",
    "        error += cvresult['test-logloss-mean'].tail(1).values[0]\n",
    "    error /= nrepeat\n",
    "\n",
    "    return{'loss':1-error, 'status': STATUS_OK }\n",
    "\n",
    "\n",
    "space ={\n",
    "        'max_depth': hp.quniform ('max_depth', 1, 14, 1),\n",
    "        'min_child_weight': hp.quniform ('min_child_weight', 1, 10, 1),\n",
    "        'subsample': hp.uniform ('subsample', 0.1, 1.),\n",
    "        'learning_rate' : hp.uniform ('learning_rate', 0.01, 0.5),\n",
    "       'colsample_bytree': hp.uniform ('colsample_bytree', 0.5, 1.),\n",
    "        'colsample_bylevel': hp.uniform ('colsample_bylevel', 0.5, 1.),\n",
    "        'gamma': hp.uniform ('gamma', 0., 1.),\n",
    "    'reg_alpha': hp.uniform ('reg_alpha', 0., 1.),\n",
    "    'reg_lambda' : hp.uniform ('reg_lambda', 0., 1.)\n",
    "    }\n",
    "\n",
    "trials = Trials()\n",
    "start_time = time()\n",
    "best = fmin(fn=objective,\n",
    "            space=space,\n",
    "            algo=tpe.suggest,\n",
    "            max_evals=100,\n",
    "            trials=trials)\n",
    "print(\"--- %s seconds ---\" % (time() - start_time))\n",
    "\n",
    "best['max_depth'] = int(best['max_depth'])\n",
    "print (best)\n",
    "\n",
    "import numpy as np\n",
    "start_time = time()\n",
    "n_estimators = []\n",
    "Error = []\n",
    "for i in range(100):\n",
    "    seed = randint(0, 100000)\n",
    "    cvresult = xgb.cv(best, dmatrix, num_boost_round=1000, nfold=5,\n",
    "                metrics='logloss', early_stopping_rounds=50, shuffle = True, seed=seed)\n",
    "    n_estimators.append(cvresult.shape[0])\n",
    "    Error.append(cvresult['test-logloss-mean'].tail(1).values[0])\n",
    "print(\"--- %s seconds ---\" % (time() - start_time))\n",
    "n_estimators_mean = np.mean(n_estimators)\n",
    "n_estimators_std = np.std(n_estimators)\n",
    "Error_mean = np.mean(Error)\n",
    "Error_std = np.std(Error)\n",
    "print(n_estimators_mean, n_estimators_std, Error_mean, Error_std)\n",
    "\n",
    "best2 = best\n",
    "best2['n_estimators'] = int(round(n_estimators_mean))\n",
    "clf = xgb.XGBClassifier(**best2)\n",
    "prediction2 = clf.fit(x,y).predict(test_df.drop('Survived',axis=1))\n",
    "\n",
    "result_df = pd.read_csv('test.csv').drop('Pclass',axis=1).drop('Name',axis=1).drop('Sex',axis=1).drop('Age',axis=1).drop('SibSp',axis=1).drop('Parch',axis=1).drop('Ticket',axis=1).drop('Fare',axis=1).drop('Cabin',axis=1).drop('Embarked',axis=1)\n",
    "result_df['Survived'] = pd.Series(data=prediction2)\n",
    "result_df = result_df.set_index('PassengerId').astype(int)\n",
    "print(result_df)\n",
    "\n",
    "result_df.to_csv(path_or_buf='Result_Titanic2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 633.4523627758026 seconds ---\n",
      "{'colsample_bylevel': 0.8227348764961984, 'colsample_bytree': 0.6545074153813044, 'gamma': 0.2883553402434034, 'learning_rate': 0.09387314161078095, 'max_depth': 5, 'min_child_weight': 2.0, 'reg_alpha': 0.17679068520207764, 'reg_lambda': 0.7885048250495174, 'subsample': 0.8616149029297578}\n",
      "--- 76.12603735923767 seconds ---\n",
      "30.14 38.6872123576 0.50046164 0.265336393349\n",
      "             Survived\n",
      "PassengerId          \n",
      "892                 0\n",
      "893                 1\n",
      "894                 0\n",
      "895                 0\n",
      "896                 1\n",
      "897                 0\n",
      "898                 1\n",
      "899                 0\n",
      "900                 1\n",
      "901                 0\n",
      "902                 0\n",
      "903                 0\n",
      "904                 1\n",
      "905                 0\n",
      "906                 1\n",
      "907                 1\n",
      "908                 0\n",
      "909                 0\n",
      "910                 0\n",
      "911                 1\n",
      "912                 0\n",
      "913                 0\n",
      "914                 1\n",
      "915                 0\n",
      "916                 1\n",
      "917                 0\n",
      "918                 1\n",
      "919                 0\n",
      "920                 0\n",
      "921                 0\n",
      "...               ...\n",
      "1280                0\n",
      "1281                0\n",
      "1282                0\n",
      "1283                1\n",
      "1284                1\n",
      "1285                0\n",
      "1286                0\n",
      "1287                1\n",
      "1288                0\n",
      "1289                1\n",
      "1290                0\n",
      "1291                0\n",
      "1292                1\n",
      "1293                0\n",
      "1294                1\n",
      "1295                0\n",
      "1296                0\n",
      "1297                0\n",
      "1298                0\n",
      "1299                0\n",
      "1300                1\n",
      "1301                1\n",
      "1302                1\n",
      "1303                1\n",
      "1304                1\n",
      "1305                0\n",
      "1306                1\n",
      "1307                0\n",
      "1308                0\n",
      "1309                1\n",
      "\n",
      "[418 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "# Try again with AUC\n",
    "evalmetrics = 'auc'\n",
    "nrepeat = 20\n",
    "repeat = range(nrepeat)\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "dmatrix = xgb.DMatrix(x.values, y.values)\n",
    "\n",
    "def objective(space):\n",
    "    dmatrix = xgb.DMatrix(x.values, y.values)\n",
    "    param={'learning_rate' : space['learning_rate'],\n",
    "     'max_depth' : int(space['max_depth']),\n",
    "     'min_child_weight' : space['min_child_weight'],\n",
    "        'gamma' : space['gamma'],\n",
    "     'subsample' : space['subsample'],\n",
    "     'colsample_bytree' :space['colsample_bytree'],\n",
    "     'nthread' : -1,\n",
    "     'colsample_bylevel' :space['colsample_bylevel'],\n",
    "        'reg_alpha' :space['reg_alpha'],\n",
    "     'reg_lambda' :space['reg_lambda']  \n",
    "          }\n",
    "    error = 0\n",
    "    for i in repeat:\n",
    "        seed = randint(0, 100000)\n",
    "        cvresult = xgb.cv(param, dmatrix, num_boost_round=1000, nfold=5,\n",
    "                metrics=evalmetrics, early_stopping_rounds=10, shuffle = True, seed=seed)\n",
    "        error += cvresult['test-auc-mean'].tail(1).values[0]\n",
    "    error /= nrepeat\n",
    "\n",
    "    return{'loss':1-error, 'status': STATUS_OK }\n",
    "\n",
    "\n",
    "space ={\n",
    "        'max_depth': hp.quniform ('max_depth', 1, 14, 1),\n",
    "        'min_child_weight': hp.quniform ('min_child_weight', 1, 10, 1),\n",
    "        'subsample': hp.uniform ('subsample', 0.1, 1.),\n",
    "        'learning_rate' : hp.uniform ('learning_rate', 0.01, 0.5),\n",
    "       'colsample_bytree': hp.uniform ('colsample_bytree', 0.5, 1.),\n",
    "        'colsample_bylevel': hp.uniform ('colsample_bylevel', 0.5, 1.),\n",
    "        'gamma': hp.uniform ('gamma', 0., 1.),\n",
    "    'reg_alpha': hp.uniform ('reg_alpha', 0., 1.),\n",
    "    'reg_lambda' : hp.uniform ('reg_lambda', 0., 1.)\n",
    "    }\n",
    "\n",
    "trials = Trials()\n",
    "start_time = time()\n",
    "best = fmin(fn=objective,\n",
    "            space=space,\n",
    "            algo=tpe.suggest,\n",
    "            max_evals=100,\n",
    "            trials=trials)\n",
    "print(\"--- %s seconds ---\" % (time() - start_time))\n",
    "\n",
    "best['max_depth'] = int(best['max_depth'])\n",
    "print (best)\n",
    "\n",
    "import numpy as np\n",
    "start_time = time()\n",
    "n_estimators = []\n",
    "Error = []\n",
    "for i in range(100):\n",
    "    seed = randint(0, 100000)\n",
    "    cvresult = xgb.cv(best, dmatrix, num_boost_round=1000, nfold=5,\n",
    "                metrics=evalmetrics, early_stopping_rounds=50, shuffle = True, seed=seed)\n",
    "    n_estimators.append(cvresult.shape[0])\n",
    "    Error.append(cvresult['test-auc-mean'].tail(1).values[0])\n",
    "print(\"--- %s seconds ---\" % (time() - start_time))\n",
    "n_estimators_mean = np.mean(n_estimators)\n",
    "n_estimators_std = np.std(n_estimators)\n",
    "Error_mean = np.mean(Error)\n",
    "Error_std = np.std(Error)\n",
    "print(n_estimators_mean, n_estimators_std, Error_mean, Error_std)\n",
    "\n",
    "best3 = best\n",
    "best3['n_estimators'] = int(round(n_estimators_mean))\n",
    "clf = xgb.XGBClassifier(**best3)\n",
    "prediction3 = clf.fit(x,y).predict(test_df.drop('Survived',axis=1))\n",
    "\n",
    "result_df = pd.read_csv('test.csv').drop('Pclass',axis=1).drop('Name',axis=1).drop('Sex',axis=1).drop('Age',axis=1).drop('SibSp',axis=1).drop('Parch',axis=1).drop('Ticket',axis=1).drop('Fare',axis=1).drop('Cabin',axis=1).drop('Embarked',axis=1)\n",
    "result_df['Survived'] = pd.Series(data=prediction3)\n",
    "result_df = result_df.set_index('PassengerId').astype(int)\n",
    "print(result_df)\n",
    "\n",
    "result_df.to_csv(path_or_buf='Result_Titanic3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40\n",
      "16\n",
      "32\n"
     ]
    }
   ],
   "source": [
    "# Compare predictions\n",
    "\n",
    "prediction1 = pd.read_csv('Result_Titanic')\n",
    "prediction2 = pd.read_csv('Result_Titanic2')\n",
    "prediction3 = pd.read_csv('Result_Titanic3')\n",
    "\n",
    "# The output is the number of different result\n",
    "print((prediction1 - prediction2).abs().sum()['Survived'])\n",
    "print((prediction1 - prediction3).abs().sum()['Survived'])\n",
    "print((prediction2 - prediction3).abs().sum()['Survived'])\n",
    "\n",
    "# I got 0.79904 for 'Result_Titanic', that is 334/418\n",
    "# I got 0.7751 for 'Result_Titanic2', that is 324/418    \n",
    "    # Thus I got 10 less in 40 different, that means 15 good changes and 25 bad changes\n",
    "# I got 0.80383, for 'Result_Titanic3', that is 336/418\n",
    "    # Thus I got 2 more in 16 different, that means 9 good changes and 7 bad changes\n",
    "    # Thus I got 12 more in 32 different, that means 22 good changes and 10 bad changes\n",
    "    \n",
    "# AUC is slightly better than accuracy, but it might just be luck. LogLoss is significantly less good."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 909.685076713562 seconds ---\n",
      "38.553 43.7054595102 0.8954749328 0.0038582568371\n",
      "             Survived\n",
      "PassengerId          \n",
      "892                 0\n",
      "893                 0\n",
      "894                 0\n",
      "895                 0\n",
      "896                 1\n",
      "897                 0\n",
      "898                 1\n",
      "899                 0\n",
      "900                 1\n",
      "901                 0\n",
      "902                 0\n",
      "903                 0\n",
      "904                 1\n",
      "905                 0\n",
      "906                 1\n",
      "907                 1\n",
      "908                 0\n",
      "909                 0\n",
      "910                 0\n",
      "911                 1\n",
      "912                 0\n",
      "913                 0\n",
      "914                 1\n",
      "915                 0\n",
      "916                 1\n",
      "917                 0\n",
      "918                 1\n",
      "919                 0\n",
      "920                 0\n",
      "921                 0\n",
      "...               ...\n",
      "1280                0\n",
      "1281                0\n",
      "1282                0\n",
      "1283                1\n",
      "1284                1\n",
      "1285                0\n",
      "1286                0\n",
      "1287                1\n",
      "1288                0\n",
      "1289                1\n",
      "1290                0\n",
      "1291                0\n",
      "1292                1\n",
      "1293                0\n",
      "1294                1\n",
      "1295                0\n",
      "1296                0\n",
      "1297                0\n",
      "1298                0\n",
      "1299                0\n",
      "1300                1\n",
      "1301                1\n",
      "1302                1\n",
      "1303                1\n",
      "1304                1\n",
      "1305                0\n",
      "1306                1\n",
      "1307                0\n",
      "1308                0\n",
      "1309                1\n",
      "\n",
      "[418 rows x 1 columns]\n",
      "             Survived\n",
      "PassengerId          \n",
      "892                 0\n",
      "893                 0\n",
      "894                 0\n",
      "895                 0\n",
      "896                 1\n",
      "897                 0\n",
      "898                 1\n",
      "899                 0\n",
      "900                 1\n",
      "901                 0\n",
      "902                 0\n",
      "903                 0\n",
      "904                 1\n",
      "905                 0\n",
      "906                 1\n",
      "907                 1\n",
      "908                 0\n",
      "909                 0\n",
      "910                 0\n",
      "911                 1\n",
      "912                 0\n",
      "913                 0\n",
      "914                 1\n",
      "915                 0\n",
      "916                 1\n",
      "917                 0\n",
      "918                 1\n",
      "919                 0\n",
      "920                 1\n",
      "921                 0\n",
      "...               ...\n",
      "1280                0\n",
      "1281                0\n",
      "1282                0\n",
      "1283                1\n",
      "1284                1\n",
      "1285                0\n",
      "1286                0\n",
      "1287                1\n",
      "1288                0\n",
      "1289                1\n",
      "1290                0\n",
      "1291                0\n",
      "1292                1\n",
      "1293                0\n",
      "1294                1\n",
      "1295                0\n",
      "1296                0\n",
      "1297                0\n",
      "1298                0\n",
      "1299                0\n",
      "1300                1\n",
      "1301                1\n",
      "1302                1\n",
      "1303                1\n",
      "1304                1\n",
      "1305                0\n",
      "1306                1\n",
      "1307                0\n",
      "1308                0\n",
      "1309                1\n",
      "\n",
      "[418 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "# Try AUC with more nestimators     We have n_estimators_mean=30.14 n_estimators_std=38.6872123576 \n",
    "# It might be better to be more precise and to go higher than the mean since the standard deviation is so large, thus we don't\n",
    "# know if the mean is close to the good number for the final training. One might hope that more estimators is better than less\n",
    "# and we have all the data for the final training while only 4/5 for the cv, so it is normal to believe that more estimators\n",
    "# could be good.\n",
    "\n",
    "evalmetrics = 'auc'\n",
    "start_time = time()\n",
    "best30 = best3\n",
    "if 'n_estimators' in best30: del best30['n_estimators']\n",
    "n_estimators = []\n",
    "Error = []\n",
    "for i in range(1000):\n",
    "    seed = randint(0, 100000)\n",
    "    cvresult = xgb.cv(best, dmatrix, num_boost_round=1000, nfold=5,\n",
    "                metrics=evalmetrics, early_stopping_rounds=50, shuffle = True, seed=seed)\n",
    "    n_estimators.append(cvresult.shape[0])\n",
    "    Error.append(cvresult['test-auc-mean'].tail(1).values[0])\n",
    "print(\"--- %s seconds ---\" % (time() - start_time))\n",
    "n_estimators_mean = np.mean(n_estimators)\n",
    "n_estimators_std = np.std(n_estimators)\n",
    "Error_mean = np.mean(Error)\n",
    "Error_std = np.std(Error)\n",
    "print(n_estimators_mean, n_estimators_std, Error_mean, Error_std)\n",
    "# Somehow, n_estimators_std do not decrease when range(1000) increases. That means it's not gaussian.\n",
    "\n",
    "best30['n_estimators'] = int(round(n_estimators_mean))\n",
    "clf = xgb.XGBClassifier(**best30)\n",
    "prediction30 = clf.fit(x,y).predict(test_df.drop('Survived',axis=1))\n",
    "\n",
    "result_df = pd.read_csv('test.csv').drop('Pclass',axis=1).drop('Name',axis=1).drop('Sex',axis=1).drop('Age',axis=1).drop('SibSp',axis=1).drop('Parch',axis=1).drop('Ticket',axis=1).drop('Fare',axis=1).drop('Cabin',axis=1).drop('Embarked',axis=1)\n",
    "result_df['Survived'] = pd.Series(data=prediction30)\n",
    "result_df = result_df.set_index('PassengerId').astype(int)\n",
    "print(result_df)\n",
    "\n",
    "result_df.to_csv(path_or_buf='Result_Titanic30')\n",
    "# I got 0.80383\n",
    "\n",
    "\n",
    "# Try with more n_estimators\n",
    "best31 = best3\n",
    "best31['n_estimators'] = int(round(n_estimators_mean + n_estimators_std))\n",
    "clf = xgb.XGBClassifier(**best31)\n",
    "prediction31 = clf.fit(x,y).predict(test_df.drop('Survived',axis=1))\n",
    "\n",
    "result_df = pd.read_csv('test.csv').drop('Pclass',axis=1).drop('Name',axis=1).drop('Sex',axis=1).drop('Age',axis=1).drop('SibSp',axis=1).drop('Parch',axis=1).drop('Ticket',axis=1).drop('Fare',axis=1).drop('Cabin',axis=1).drop('Embarked',axis=1)\n",
    "result_df['Survived'] = pd.Series(data=prediction31)\n",
    "result_df = result_df.set_index('PassengerId').astype(int)\n",
    "print(result_df)\n",
    "\n",
    "result_df.to_csv(path_or_buf='Result_Titanic31')\n",
    "# I got 0.78947\n",
    "\n",
    "# It seems it's not the case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method fit in module xgboost.sklearn:\n",
      "\n",
      "fit(X, y, sample_weight=None, eval_set=None, eval_metric=None, early_stopping_rounds=None, verbose=True) method of xgboost.sklearn.XGBClassifier instance\n",
      "    Fit gradient boosting classifier\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    X : array_like\n",
      "        Feature matrix\n",
      "    y : array_like\n",
      "        Labels\n",
      "    sample_weight : array_like\n",
      "        Weight for each instance\n",
      "    eval_set : list, optional\n",
      "        A list of (X, y) pairs to use as a validation set for\n",
      "        early-stopping\n",
      "    eval_metric : str, callable, optional\n",
      "        If a str, should be a built-in evaluation metric to use. See\n",
      "        doc/parameter.md. If callable, a custom evaluation metric. The call\n",
      "        signature is func(y_predicted, y_true) where y_true will be a\n",
      "        DMatrix object such that you may need to call the get_label\n",
      "        method. It must return a str, value pair where the str is a name\n",
      "        for the evaluation and value is the value of the evaluation\n",
      "        function. This objective is always minimized.\n",
      "    early_stopping_rounds : int, optional\n",
      "        Activates early stopping. Validation error needs to decrease at\n",
      "        least every <early_stopping_rounds> round(s) to continue training.\n",
      "        Requires at least one item in evals.  If there's more than one,\n",
      "        will use the last. Returns the model from the last iteration\n",
      "        (not the best one). If early stopping occurs, the model will\n",
      "        have three additional fields: bst.best_score, bst.best_iteration\n",
      "        and bst.best_ntree_limit.\n",
      "        (Use bst.best_ntree_limit to get the correct value if num_parallel_tree\n",
      "        and/or num_class appears in the parameters)\n",
      "    verbose : bool\n",
      "        If `verbose` and an evaluation set is used, writes the evaluation\n",
      "        metric measured on the validation set to stderr.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(clf.fit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n"
     ]
    }
   ],
   "source": [
    "# Maybe always use early stopping\n",
    "# It has however the bad effect to make that we can't use all the data.\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.1)\n",
    "eval_set=[(X_test, y_test)]\n",
    "bestES=best3 # Use the best set of parameter\n",
    "bestES['n_estimators'] = 1000 # Need to be large so the stopping is by early stopping\n",
    "clf = xgb.XGBClassifier(**bestES)\n",
    "clf.fit(X_train,y_train, eval_set=eval_set,early_stopping_rounds=50, eval_metric='auc', verbose=False)\n",
    "prediction4 = clf.predict(test_df.drop('Survived',axis=1))\n",
    "\n",
    "result_df = pd.read_csv('test.csv').drop('Pclass',axis=1).drop('Name',axis=1).drop('Sex',axis=1).drop('Age',axis=1).drop('SibSp',axis=1).drop('Parch',axis=1).drop('Ticket',axis=1).drop('Fare',axis=1).drop('Cabin',axis=1).drop('Embarked',axis=1)\n",
    "result_df['Survived'] = pd.Series(data=prediction4)\n",
    "result_df = result_df.set_index('PassengerId').astype(int)\n",
    "\n",
    "result_df.to_csv(path_or_buf='Result_Titanic4')\n",
    "\n",
    "prediction4 = pd.read_csv('Result_Titanic4')\n",
    "print((prediction4 - prediction3).abs().sum()['Survived'])\n",
    "\n",
    "# I got 0.78469"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 412.9174773693085 seconds ---\n",
      "{'colsample_bylevel': 0.9235200762181645, 'colsample_bytree': 0.822453160665294, 'gamma': 0.7791936249481772, 'learning_rate': 0.1955450438296506, 'max_depth': 6, 'min_child_weight': 1.0, 'reg_alpha': 0.7308280629929902, 'reg_lambda': 0.4987932979468526, 'subsample': 0.8200814329518703}\n",
      "{'colsample_bylevel': 0.9235200762181645, 'colsample_bytree': 0.822453160665294, 'gamma': 0.7791936249481772, 'learning_rate': 0.1955450438296506, 'max_depth': 6, 'min_child_weight': 1.0, 'reg_alpha': 0.7308280629929902, 'reg_lambda': 0.4987932979468526, 'subsample': 0.8200814329518703}\n",
      "--- 155.65548515319824 seconds ---\n"
     ]
    },
    {
     "ename": "IndexingError",
     "evalue": "Unalignable boolean Series key provided",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexingError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-c1455e251f9b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[0mresult_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'test.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Pclass'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Name'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Age'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'SibSp'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Parch'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Ticket'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Fare'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Cabin'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Embarked'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[1;31m#result_df['Survived'] = pd.Series(data=prediction)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m \u001b[0mresult_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mix\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Survived'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mresult_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Sex'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSeries\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpredictionM\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    127\u001b[0m \u001b[0mresult_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mix\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Survived'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mresult_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Sex'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSeries\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpredictionF\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[0mresult_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mresult_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'PassengerId'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Sex'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\nicol\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m__setitem__\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m    138\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m             \u001b[0mkey\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_apply_if_callable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 140\u001b[0;31m         \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_setitem_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    141\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_setitem_with_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\nicol\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m_get_setitem_indexer\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_convert_tuple\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mis_setter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_convert_range\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mis_setter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\nicol\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m_convert_tuple\u001b[0;34m(self, key, is_setter)\u001b[0m\n\u001b[1;32m    182\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mk\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 184\u001b[0;31m                 \u001b[0midx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_convert_to_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mis_setter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mis_setter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    185\u001b[0m                 \u001b[0mkeyidx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkeyidx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\nicol\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m_convert_to_indexer\u001b[0;34m(self, obj, axis, is_setter)\u001b[0m\n\u001b[1;32m   1184\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mis_list_like_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1185\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mis_bool_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1186\u001b[0;31m                 \u001b[0mobj\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_bool_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1187\u001b[0m                 \u001b[0minds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnonzero\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1188\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0minds\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\nicol\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36mcheck_bool_indexer\u001b[0;34m(ax, key)\u001b[0m\n\u001b[1;32m   1815\u001b[0m         \u001b[0mmask\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0misnull\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1816\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mmask\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0many\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1817\u001b[0;31m             \u001b[1;32mraise\u001b[0m \u001b[0mIndexingError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Unalignable boolean Series key provided'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1818\u001b[0m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbool\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1819\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mis_sparse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexingError\u001b[0m: Unalignable boolean Series key provided"
     ]
    }
   ],
   "source": [
    "# Try to separate first between male and female since they are very different samples.\n",
    "\n",
    "import xgboost as xgb\n",
    "from hyperopt import hp, fmin, tpe, STATUS_OK, Trials\n",
    "from time import time\n",
    "from random import randint\n",
    "import numpy as np\n",
    "\n",
    "trainM_df = train_df[train_df['Sex']==0]\n",
    "trainF_df = train_df[train_df['Sex']==1]\n",
    "testM_df = test_df[test_df['Sex']==0]\n",
    "testF_df = test_df[test_df['Sex']==1]\n",
    "\n",
    "xM = trainM_df.drop('Survived',axis=1)\n",
    "yM = trainM_df['Survived']\n",
    "xF= trainF_df.drop('Survived',axis=1)\n",
    "yF = trainF_df['Survived']\n",
    "\n",
    "dmatrixM = xgb.DMatrix(xM.values, yM.values)\n",
    "dmatrixF = xgb.DMatrix(xF.values, yF.values)\n",
    "\n",
    "evalmetrics = 'auc'\n",
    "nrepeat = 20\n",
    "repeat = range(nrepeat)\n",
    "def objectiveM(space):\n",
    "    param={'learning_rate' : space['learning_rate'],\n",
    "     'max_depth' : int(space['max_depth']),\n",
    "     'min_child_weight' : space['min_child_weight'],\n",
    "        'gamma' : space['gamma'],\n",
    "     'subsample' : space['subsample'],\n",
    "     'colsample_bytree' :space['colsample_bytree'],\n",
    "     'nthread' : -1,\n",
    "     'colsample_bylevel' :space['colsample_bylevel'],\n",
    "        'reg_alpha' :space['reg_alpha'],\n",
    "     'reg_lambda' :space['reg_lambda']  \n",
    "          }\n",
    "    error = 0\n",
    "    for i in repeat:\n",
    "        seed = randint(0, 100000)\n",
    "        cvresult = xgb.cv(param, dmatrixM, num_boost_round=1000, nfold=5,\n",
    "                metrics=evalmetrics, early_stopping_rounds=10, shuffle = True, seed=seed)\n",
    "        error += cvresult['test-auc-mean'].tail(1).values[0]\n",
    "    error /= nrepeat\n",
    "\n",
    "    return{'loss':1-error, 'status': STATUS_OK }\n",
    "def objectiveF(space):\n",
    "    param={'learning_rate' : space['learning_rate'],\n",
    "     'max_depth' : int(space['max_depth']),\n",
    "     'min_child_weight' : space['min_child_weight'],\n",
    "        'gamma' : space['gamma'],\n",
    "     'subsample' : space['subsample'],\n",
    "     'colsample_bytree' :space['colsample_bytree'],\n",
    "     'nthread' : -1,\n",
    "     'colsample_bylevel' :space['colsample_bylevel'],\n",
    "        'reg_alpha' :space['reg_alpha'],\n",
    "     'reg_lambda' :space['reg_lambda']  \n",
    "          }\n",
    "    error = 0\n",
    "    for i in repeat:\n",
    "        seed = randint(0, 100000)\n",
    "        cvresult = xgb.cv(param, dmatrixF, num_boost_round=1000, nfold=5,\n",
    "                metrics=evalmetrics, early_stopping_rounds=10, shuffle = True, seed=seed)\n",
    "        error += cvresult['test-auc-mean'].tail(1).values[0]\n",
    "    error /= nrepeat\n",
    "\n",
    "    return{'loss':1-error, 'status': STATUS_OK }\n",
    "\n",
    "space ={\n",
    "        'max_depth': hp.quniform ('max_depth', 1, 14, 1),\n",
    "        'min_child_weight': hp.quniform ('min_child_weight', 1, 10, 1),\n",
    "        'subsample': hp.uniform ('subsample', 0.1, 1.),\n",
    "        'learning_rate' : hp.uniform ('learning_rate', 0.01, 0.5),\n",
    "       'colsample_bytree': hp.uniform ('colsample_bytree', 0.5, 1.),\n",
    "        'colsample_bylevel': hp.uniform ('colsample_bylevel', 0.5, 1.),\n",
    "        'gamma': hp.uniform ('gamma', 0., 1.),\n",
    "    'reg_alpha': hp.uniform ('reg_alpha', 0., 1.),\n",
    "    'reg_lambda' : hp.uniform ('reg_lambda', 0., 1.)\n",
    "    }\n",
    "\n",
    "trials = Trials()\n",
    "start_time = time()\n",
    "bestM = fmin(fn=objectiveM,\n",
    "            space=space,\n",
    "            algo=tpe.suggest,\n",
    "            max_evals=100,\n",
    "            trials=trials)\n",
    "bestF = fmin(fn=objectiveF,\n",
    "            space=space,\n",
    "            algo=tpe.suggest,\n",
    "            max_evals=100,\n",
    "            trials=trials)\n",
    "print(\"--- %s seconds ---\" % (time() - start_time))\n",
    "\n",
    "bestM['max_depth'] = int(bestM['max_depth'])\n",
    "bestF['max_depth'] = int(bestF['max_depth'])\n",
    "print (bestM)\n",
    "print (bestF)\n",
    "\n",
    "start_time = time()\n",
    "n_estimators = []\n",
    "for i in range(100):\n",
    "    seed = randint(0, 100000)\n",
    "    cvresult = xgb.cv(bestM, dmatrixM, num_boost_round=1000, nfold=5,\n",
    "                metrics=evalmetrics, early_stopping_rounds=50, shuffle = True, seed=seed)\n",
    "    n_estimators.append(cvresult.shape[0])\n",
    "n_estimators_meanM = np.mean(n_estimators)\n",
    "    \n",
    "for i in range(100):\n",
    "    seed = randint(0, 100000)\n",
    "    cvresult = xgb.cv(bestF, dmatrixF, num_boost_round=1000, nfold=5,\n",
    "                metrics=evalmetrics, early_stopping_rounds=50, shuffle = True, seed=seed)\n",
    "    n_estimators.append(cvresult.shape[0])\n",
    "n_estimators_meanF = np.mean(n_estimators)\n",
    "    \n",
    "print(\"--- %s seconds ---\" % (time() - start_time))\n",
    "\n",
    "bestM['n_estimators'] = int(round(n_estimators_meanM))\n",
    "bestF['n_estimators'] = int(round(n_estimators_meanF))\n",
    "clf = xgb.XGBClassifier(**bestM)\n",
    "predictionM = clf.fit(xM,yM).predict(testM_df.drop('Survived',axis=1))\n",
    "clf = xgb.XGBClassifier(**bestF)\n",
    "predictionF = clf.fit(xF,yF).predict(testF_df.drop('Survived',axis=1))\n",
    "\n",
    "result_df = pd.read_csv('test.csv').drop('Pclass',axis=1).drop('Name',axis=1).drop('Age',axis=1).drop('SibSp',axis=1).drop('Parch',axis=1).drop('Ticket',axis=1).drop('Fare',axis=1).drop('Cabin',axis=1).drop('Embarked',axis=1)\n",
    "result_df['Survived'] = result_df['Sex']\n",
    "result_df['Survived'].loc[result_df['Sex']=='male'] = predictionM\n",
    "result_df['Survived'].loc[result_df['Sex']=='female'] = predictionF\n",
    "result_df = result_df.set_index('PassengerId').drop('Sex',axis=1).astype(int)\n",
    "print(result_df)\n",
    "\n",
    "result_df.to_csv(path_or_buf='Result_Titanic6')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             Survived\n",
      "PassengerId          \n",
      "892                 0\n",
      "893                 0\n",
      "894                 0\n",
      "895                 0\n",
      "896                 1\n",
      "897                 0\n",
      "898                 1\n",
      "899                 0\n",
      "900                 1\n",
      "901                 0\n",
      "902                 0\n",
      "903                 0\n",
      "904                 1\n",
      "905                 0\n",
      "906                 1\n",
      "907                 1\n",
      "908                 0\n",
      "909                 0\n",
      "910                 0\n",
      "911                 0\n",
      "912                 0\n",
      "913                 0\n",
      "914                 1\n",
      "915                 0\n",
      "916                 1\n",
      "917                 0\n",
      "918                 1\n",
      "919                 0\n",
      "920                 1\n",
      "921                 0\n",
      "...               ...\n",
      "1280                0\n",
      "1281                0\n",
      "1282                0\n",
      "1283                1\n",
      "1284                1\n",
      "1285                0\n",
      "1286                0\n",
      "1287                1\n",
      "1288                0\n",
      "1289                1\n",
      "1290                0\n",
      "1291                0\n",
      "1292                1\n",
      "1293                0\n",
      "1294                1\n",
      "1295                0\n",
      "1296                0\n",
      "1297                0\n",
      "1298                0\n",
      "1299                0\n",
      "1300                1\n",
      "1301                1\n",
      "1302                1\n",
      "1303                1\n",
      "1304                0\n",
      "1305                0\n",
      "1306                1\n",
      "1307                0\n",
      "1308                0\n",
      "1309                1\n",
      "\n",
      "[418 rows x 1 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nicol\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py:141: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self._setitem_with_indexer(indexer, value)\n"
     ]
    }
   ],
   "source": [
    "result_df = pd.read_csv('test.csv').drop('Pclass',axis=1).drop('Name',axis=1).drop('Age',axis=1).drop('SibSp',axis=1).drop('Parch',axis=1).drop('Ticket',axis=1).drop('Fare',axis=1).drop('Cabin',axis=1).drop('Embarked',axis=1)\n",
    "result_df['Survived'] = result_df['Sex']\n",
    "result_df['Survived'].loc[result_df['Sex']=='male'] = predictionM\n",
    "result_df['Survived'].loc[result_df['Sex']=='female'] = predictionF\n",
    "result_df = result_df.set_index('PassengerId').drop('Sex',axis=1).astype(int)\n",
    "print(result_df)\n",
    "\n",
    "result_df.to_csv(path_or_buf='Result_Titanic6')\n",
    "\n",
    "# I got 0.79426\n",
    "# So it wasn't a good idea."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 1369.4690580368042 seconds ---\n",
      "{'colsample_bylevel': 0.822266637784425, 'colsample_bytree': 0.7688159874607244, 'gamma': 0.24141815239315229, 'learning_rate': 0.1519318808587794, 'max_depth': 5, 'min_child_weight': 8.0, 'reg_alpha': 0.8767988943536921, 'reg_lambda': 0.29774614494309326, 'subsample': 0.7056111963447809}\n",
      "{'colsample_bylevel': 0.8762569525817361, 'colsample_bytree': 0.7754800031185707, 'gamma': 0.2571815526593193, 'learning_rate': 0.20786990229752467, 'max_depth': 6, 'min_child_weight': 5.0, 'reg_alpha': 0.5395288974302214, 'reg_lambda': 0.3170365728183162, 'subsample': 0.5669572593505213}\n",
      "--- 9916.568351268768 seconds ---\n",
      "35.21 32.69\n",
      "17.0\n"
     ]
    }
   ],
   "source": [
    "# Add Cabin features\n",
    "\n",
    "train_df = pd.read_csv('train.csv')\n",
    "test_df = pd.read_csv('test.csv')\n",
    "combine_df = train_df.append(test_df)\n",
    "\n",
    "combine_df = combine_df.join(combine_df.groupby(by='Ticket').size().rename('Ticket_count'),on='Ticket')\n",
    "combine_df['FarePerPerson']=combine_df['Fare']/combine_df['Ticket_count']\n",
    "combine_df['Family'] = combine_df['SibSp'] + combine_df['Parch']\n",
    "combine_df['Sex'] = combine_df['Sex'].map( {'female': 1, 'male': 0} )\n",
    "# PClass is not a real number, let's transform it into dummies.\n",
    "combine_df['Pclass'] = combine_df['Pclass'].replace({1 : '1st',2 : '2nd',3 : '3rd'})\n",
    "\n",
    "# Create title row.\n",
    "def fun0(x):\n",
    "    tmp = (x.split(',')[1]).split()[0]\n",
    "    if tmp in ['Mr.','Master.','Dr.','Rev.']:\n",
    "        return tmp\n",
    "    elif tmp in ['Miss.', 'Mlle.', 'Ms.']:\n",
    "        return 'Miss.'\n",
    "    elif tmp in ['Mrs.', 'Mme.']:\n",
    "        return 'Mrs.'\n",
    "    else:\n",
    "        return 'Other'\n",
    "combine_df['title'] = combine_df['Name'].apply(fun0)\n",
    "\n",
    "# Create same ticket survival\n",
    "tmp1 = (combine_df.groupby(by='Ticket').count())['Survived']\n",
    "tmp2 = (combine_df.groupby(by='Ticket').sum())['Survived']\n",
    "combine_df['tmp1'] = combine_df['Ticket'].replace(to_replace=tmp1)   # Number of non_nan survived with same ticket\n",
    "combine_df['tmp2'] = combine_df['Ticket'].replace(to_replace=tmp2)   # Sum of non_nan survived with same ticket\n",
    "\n",
    "def f(row):\n",
    "    if np.isnan(row['Survived']): # test\n",
    "        if row['tmp1'] == 0 :\n",
    "            val = np.nan\n",
    "        else:\n",
    "            val = row['tmp2'] /row['tmp1']\n",
    "    else: # train\n",
    "        if row['tmp1'] == 1 :\n",
    "            val = np.nan\n",
    "        else:\n",
    "            val = (row['tmp2']-row['Survived']) / (row['tmp1']-1)\n",
    "    return val\n",
    "combine_df['SameTicketSurvival'] = combine_df.apply(f, axis=1)\n",
    "combine_df = combine_df.drop('tmp1',axis=1).drop('tmp2',axis=1)\n",
    "\n",
    "# Create same last name survival\n",
    "combine_df['LastName'] = combine_df['Name'].apply(lambda x : (x.split(',')[0]))\n",
    "tmp1 = (combine_df.groupby(by='LastName').count())['Survived']\n",
    "tmp2 = (combine_df.groupby(by='LastName').sum())['Survived']\n",
    "combine_df['tmp1'] = combine_df['LastName'].replace(to_replace=tmp1)   # Number of non_nan survived with same ticket\n",
    "combine_df['tmp2'] = combine_df['LastName'].replace(to_replace=tmp2)   # Sum of non_nan survived with same ticket\n",
    "combine_df['LastNameSurvival'] = combine_df.apply(f, axis=1)\n",
    "combine_df = (combine_df.drop('tmp1',axis=1).drop('tmp2',axis=1).drop('PassengerId',axis=1).drop('Name',axis=1))\n",
    "\n",
    "# Use only first cabin\n",
    "combine_df['Cabin'] = combine_df['Cabin'].apply(lambda x: x.split()[0] if x == x else x)\n",
    "\n",
    "# Fill cabin with last name\n",
    "for s in combine_df.groupby(by='Ticket')['Cabin']:\n",
    "    s1=s[1]  # s[0] is the Last Name\n",
    "    s1.fillna(method='ffill', inplace=True)\n",
    "    s1.fillna(method='bfill', inplace=True)\n",
    "    \n",
    "# Fill cabin with ticket\n",
    "for s in combine_df.groupby(by='Ticket')['Cabin']:\n",
    "    s1=s[1]  # s[0] is the ticket number\n",
    "    s1.fillna(method='ffill', inplace=True)\n",
    "    s1.fillna(method='bfill', inplace=True)\n",
    "    \n",
    "# Split cabin into letter and integer\n",
    "combine_df['CabinLetter'] = combine_df['Cabin'].apply(lambda x: x[0] if x == x else x)\n",
    "def tmp(x):\n",
    "    if x != x :\n",
    "        return x\n",
    "    elif len(x)>1:\n",
    "        return float(x[1:])\n",
    "    else:\n",
    "        return np.nan\n",
    "combine_df['CabinNumber'] = combine_df['Cabin'].apply(tmp)\n",
    "\n",
    "# Drop features with too many categories\n",
    "combine_df = combine_df.drop('LastName',axis=1).drop('Ticket',axis=1).drop('Cabin',axis=1)\n",
    "    \n",
    "# Create dummies for every string variable\n",
    "combine_df = pd.get_dummies(combine_df)\n",
    "\n",
    "# Split Other between gender\n",
    "combine_df['OtherFemale'] = combine_df['title_Other'] * combine_df['Sex'] + combine_df['title_Dr.'] * combine_df['Sex']\n",
    "combine_df['OtherMale'] = combine_df['title_Other'] - combine_df['OtherFemale']\n",
    "\n",
    "# Split classes between gender\n",
    "combine_df['1F'] = combine_df['Pclass_1st'] * combine_df['Sex']\n",
    "combine_df['2F'] = combine_df['Pclass_2nd'] * combine_df['Sex']\n",
    "combine_df['3F'] = combine_df['Pclass_3rd'] * combine_df['Sex']\n",
    "combine_df['1M'] = combine_df['Pclass_1st'] - combine_df['1F']\n",
    "combine_df['2M'] = combine_df['Pclass_2nd'] - combine_df['2F']\n",
    "combine_df['3M'] = combine_df['Pclass_3rd'] - combine_df['3F']\n",
    "\n",
    "# Separate in train and test\n",
    "train_df = combine_df[pd.notnull(combine_df['Survived'])]\n",
    "test_df = combine_df[False == pd.notnull(combine_df['Survived'])]\n",
    "x = train_df.drop('Survived',axis=1)\n",
    "y = train_df['Survived']\n",
    "\n",
    "nrepeat = 20\n",
    "repeat = range(nrepeat)\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "dmatrix = xgb.DMatrix(x.values, y.values)\n",
    "\n",
    "def objective(space):\n",
    "    param={'learning_rate' : space['learning_rate'],\n",
    "     'max_depth' : int(space['max_depth']),\n",
    "     'min_child_weight' : space['min_child_weight'],\n",
    "        'gamma' : space['gamma'],\n",
    "     'subsample' : space['subsample'],\n",
    "     'colsample_bytree' :space['colsample_bytree'],\n",
    "     'nthread' : -1,\n",
    "     'colsample_bylevel' :space['colsample_bylevel'],\n",
    "        'reg_alpha' :space['reg_alpha'],\n",
    "     'reg_lambda' :space['reg_lambda']  \n",
    "          }\n",
    "    error = 0\n",
    "    for i in repeat:\n",
    "        seed = randint(0, 100000)\n",
    "        cvresult = xgb.cv(param, dmatrix, num_boost_round=1000, nfold=5,\n",
    "                metrics=evalmetrics, early_stopping_rounds=10, shuffle = True, seed=seed)\n",
    "        if evalmetrics == 'auc':\n",
    "            error += 1-cvresult['test-auc-mean'].tail(1).values[0]\n",
    "        elif evalmetrics == 'error':\n",
    "            error += cvresult['test-error-mean'].tail(1).values[0]\n",
    "    error /= nrepeat\n",
    "\n",
    "    return error\n",
    "\n",
    "\n",
    "space ={\n",
    "        'max_depth': hp.quniform ('max_depth', 1, 14, 1),\n",
    "        'min_child_weight': hp.quniform ('min_child_weight', 1, 10, 1),\n",
    "        'subsample': hp.uniform ('subsample', 0.1, 1.),\n",
    "        'learning_rate' : hp.uniform ('learning_rate', 0.01, 0.5),\n",
    "       'colsample_bytree': hp.uniform ('colsample_bytree', 0.5, 1.),\n",
    "        'colsample_bylevel': hp.uniform ('colsample_bylevel', 0.5, 1.),\n",
    "        'gamma': hp.uniform ('gamma', 0., 1.),\n",
    "    'reg_alpha': hp.uniform ('reg_alpha', 0., 1.),\n",
    "    'reg_lambda' : hp.uniform ('reg_lambda', 0., 1.)\n",
    "    }\n",
    "\n",
    "start_time = time()\n",
    "evalmetrics = 'auc'\n",
    "bestAUC = fmin(fn=objective,\n",
    "            space=space,\n",
    "            algo=tpe.suggest,\n",
    "            max_evals=100)\n",
    "evalmetrics = 'error'\n",
    "bestError = fmin(fn=objective,\n",
    "            space=space,\n",
    "            algo=tpe.suggest,\n",
    "            max_evals=100)\n",
    "print(\"--- %s seconds ---\" % (time() - start_time))\n",
    "\n",
    "bestAUC['max_depth'] = int(bestAUC['max_depth'])\n",
    "print (bestAUC)\n",
    "bestError['max_depth'] = int(bestError['max_depth'])\n",
    "print (bestError)\n",
    "\n",
    "import numpy as np\n",
    "start_time = time()\n",
    "n_estimatorsAUC = []\n",
    "n_estimatorsError = []\n",
    "for i in range(100):\n",
    "    seed = randint(0, 100000)\n",
    "    cvresult = xgb.cv(bestAUC, dmatrix, num_boost_round=1000, nfold=5,\n",
    "                metrics=evalmetrics, early_stopping_rounds=50, shuffle = True, seed=seed)\n",
    "    n_estimatorsAUC.append(cvresult.shape[0])\n",
    "for i in range(100):\n",
    "    seed = randint(0, 100000)\n",
    "    cvresult = xgb.cv(bestError, dmatrix, num_boost_round=1000, nfold=5,\n",
    "                metrics=evalmetrics, early_stopping_rounds=50, shuffle = True, seed=seed)\n",
    "    n_estimatorsError.append(cvresult.shape[0])\n",
    "print(\"--- %s seconds ---\" % (time() - start_time))\n",
    "n_estimatorsAUC_mean = np.mean(n_estimatorsAUC)\n",
    "n_estimatorsError_mean = np.mean(n_estimatorsError)\n",
    "print(n_estimatorsAUC_mean, n_estimatorsError_mean)\n",
    "\n",
    "bestAUC['n_estimators'] = int(round(n_estimatorsAUC_mean))\n",
    "clf = xgb.XGBClassifier(**bestAUC)\n",
    "predictionAUC = clf.fit(x,y).predict(test_df.drop('Survived',axis=1))\n",
    "result_df = pd.read_csv('test.csv').drop('Pclass',axis=1).drop('Name',axis=1).drop('Sex',axis=1).drop('Age',axis=1).drop('SibSp',axis=1).drop('Parch',axis=1).drop('Ticket',axis=1).drop('Fare',axis=1).drop('Cabin',axis=1).drop('Embarked',axis=1)\n",
    "result_df['Survived'] = pd.Series(data=predictionAUC)\n",
    "result_df = result_df.set_index('PassengerId').astype(int)\n",
    "result_df.to_csv(path_or_buf='Result_TitanicAUC')\n",
    "\n",
    "bestError['n_estimators'] = int(round(n_estimatorsError_mean))\n",
    "clf = xgb.XGBClassifier(**bestError)\n",
    "predictionError = clf.fit(x,y).predict(test_df.drop('Survived',axis=1))\n",
    "result_df = pd.read_csv('test.csv').drop('Pclass',axis=1).drop('Name',axis=1).drop('Sex',axis=1).drop('Age',axis=1).drop('SibSp',axis=1).drop('Parch',axis=1).drop('Ticket',axis=1).drop('Fare',axis=1).drop('Cabin',axis=1).drop('Embarked',axis=1)\n",
    "result_df['Survived'] = pd.Series(data=predictionError)\n",
    "result_df = result_df.set_index('PassengerId').astype(int)\n",
    "result_df.to_csv(path_or_buf='Result_TitanicError')\n",
    "\n",
    "# Differences between AUC and Error\n",
    "print((pd.Series(data=predictionAUC) - pd.Series(data=predictionError)).abs().sum())\n",
    "\n",
    "# I got 0.77990 for AUC\n",
    "# I got 0.79426 for Error\n",
    "\n",
    "# Still less than initial answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Stack classifier\n",
    "# Rather than find the n_estimators_mean and then use it on all data. \n",
    "# I train 5 classifier with 20% data as CV to do early stopping. Then they vote.\n",
    "\n",
    "bestES={'colsample_bylevel': 0.8227348764961984, 'colsample_bytree': 0.6545074153813044, 'gamma': 0.2883553402434034, 'learning_rate': 0.09387314161078095, 'max_depth': 5, 'min_child_weight': 2.0, 'reg_alpha': 0.17679068520207764, 'reg_lambda': 0.7885048250495174, 'subsample': 0.8616149029297578}\n",
    "# Use the best set of parameter which is the first one with AUC\n",
    "bestES['n_estimators'] = 1000 # Need to be large so the stopping is by early stopping\n",
    "\n",
    "kf = KFold(n_splits=5)\n",
    "prediction = np.zeros(len(test_df))\n",
    "for train_index, cv_index in kf.split(x):\n",
    "    eval_set=[(x.iloc[cv_index], y.iloc[cv_index])]\n",
    "    clf = xgb.XGBClassifier(**bestES)\n",
    "    clf.fit(x.iloc[train_index],y.iloc[train_index], eval_set=eval_set,early_stopping_rounds=50, eval_metric='error', verbose=False)\n",
    "    prediction += np.array(clf.predict(test_df.drop('Survived',axis=1)))\n",
    "prediction = (prediction/5).round().astype(int)\n",
    "\n",
    "result_df = pd.read_csv('test.csv').drop('Pclass',axis=1).drop('Name',axis=1).drop('Sex',axis=1).drop('Age',axis=1).drop('SibSp',axis=1).drop('Parch',axis=1).drop('Ticket',axis=1).drop('Fare',axis=1).drop('Cabin',axis=1).drop('Embarked',axis=1)\n",
    "result_df['Survived'] = pd.Series(data=prediction)\n",
    "result_df = result_df.set_index('PassengerId').astype(int)\n",
    "result_df.to_csv(path_or_buf='Result_Titanic7')\n",
    "\n",
    "# I got 0.78947"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# It seems I can't do better than my initial 0.80383.\n",
    "# This result is good considering that there is no reason to believe that survival is perfectly determined by socio-economical\n",
    "# status.\n",
    "# There are a lot of people who got better results, but some of them, and certainly all the 100%, cheated. Which is easy since\n",
    "# one can directly check on the net any name on the list to know their survival status."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
